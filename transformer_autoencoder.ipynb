{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db2b169-949e-4609-a4f6-51a67fac08f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from Environment.SQL.SQL import SQL\n",
    "from Environment.Tokens_Actions.Basic_Block.KeywordRepresentation_Token import (\n",
    "    KeywordRepresentation_Token,\n",
    ")\n",
    "from Environment.Tokens_Actions.Basic_Block.Comma_Token import Comma_Token\n",
    "from Environment.Tokens_Actions.Basic_Block.Comment_Token import Comment_Token\n",
    "from Environment.Tokens_Actions.Basic_Block.Paranthesis_Token import Paranthesis_Token\n",
    "from Environment.Tokens_Actions.Basic_Block.Quote_Token import Quote_Token\n",
    "from Environment.Tokens_Actions.Basic_Block.Whitespace_Token import Whitespace_Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01259e10-17a7-4eee-8a64-e3d49ce24c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_stmt_path = \"generic_synatx_statment.dataset\"\n",
    "\n",
    "with open(generic_stmt_path, \"rb\") as f:\n",
    "    generic_sqls = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb754fcd-0045-4652-9a80-858171c5c563",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = KeywordRepresentation_Token.reserved_keywords\n",
    "# operators\n",
    "operators = [\"op\"]\n",
    "# comma\n",
    "comma = Comma_Token.comma_types\n",
    "# comment\n",
    "comments = list(Comment_Token.comment_type_mapping.values())\n",
    "# hex\n",
    "hex = [\"hex\"]\n",
    "# string\n",
    "string = [\"str\"]\n",
    "# id\n",
    "id = [\"id\"]\n",
    "# number\n",
    "number = [\"num\"]\n",
    "# parenthesis\n",
    "paranthesis = list(Paranthesis_Token.paranthesis_type_mapping.values())\n",
    "# quotes\n",
    "quotes = list(Quote_Token.quote_type_mapping.values())\n",
    "# whitespace\n",
    "whitespace = list(Whitespace_Token.whitespace_type_mapping.values())\n",
    "# padding\n",
    "pad_token = [\"pad\"]\n",
    "# starting and ending token\n",
    "start_end_token = [\"sos\", \"eos\"]\n",
    "\n",
    "tokens_embedding = [\n",
    "    *pad_token,\n",
    "    *keywords,\n",
    "    *operators,\n",
    "    *comma,\n",
    "    *comments,\n",
    "    *hex,\n",
    "    *string,\n",
    "    *id,\n",
    "    *number,\n",
    "    *paranthesis,\n",
    "    *quotes,\n",
    "    *whitespace,\n",
    "    *start_end_token,\n",
    "]\n",
    "token_to_idx = {c: i for i, c in enumerate(tokens_embedding)}\n",
    "idx_to_token = {i: c for i, c in enumerate(tokens_embedding)}\n",
    "\n",
    "\n",
    "\n",
    "def tokenizer(sql: SQL):\n",
    "    result = []\n",
    "\n",
    "    for current_token in sql.get_tokens().flat_idx_tokens_list():\n",
    "        result.append(\n",
    "            token_to_idx[str(current_token).casefold()]\n",
    "        )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13dba774-fbf1-43c6-86af-e9150227df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_SEQ_LEN = 41    # Maximum length of SQL queries\n",
    "EMBEDDING_DIM = 128  # Embedding dimension\n",
    "HIDDEN_DIM = 1024     # Hidden layer size in the Transformer\n",
    "NUM_HEADS = 4        # Number of attention heads\n",
    "NUM_LAYERS = 2       # Number of Transformer layers\n",
    "DROPOUT = 0.1        # Dropout rate\n",
    "EPOCHS = 10          # Number of epochs\n",
    "BATCH_SIZE = 64      # Batch size\n",
    "LEARNING_RATE = 1e-4 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37335597-8bbe-41cb-a0b1-a64fe6f9fd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLDataset(Dataset):\n",
    "    def __init__(self, queries, tokenizer, max_len=MAX_SEQ_LEN):\n",
    "        self.queries = queries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        embeddings = []\n",
    "        for query in queries:\n",
    "            tokenized_query = self.tokenizer(query)\n",
    "            tokenized_query = tokenized_query[:self.max_len]\n",
    "            padding = [0] * (self.max_len - len(tokenized_query))\n",
    "            input_sequence = torch.tensor(tokenized_query + padding)\n",
    "            embeddings.append(input_sequence)\n",
    "\n",
    "        self.embeddings = torch.stack(embeddings)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_sequence = self.embeddings[idx]\n",
    "        return input_sequence, input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80eefee-2526-40e0-afb7-0296eab36ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_heads, num_layers, max_len, dropout):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.positional_encoding = self._generate_positional_encoding(max_len, embedding_dim)\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout),\n",
    "            num_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(embedding_dim, num_heads, hidden_dim, dropout),\n",
    "            num_layers\n",
    "        )\n",
    "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def _generate_positional_encoding(self, max_len, embedding_dim):\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-torch.log(torch.tensor(10000.0)) / embedding_dim))\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedding = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt_embedding = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "\n",
    "        encoded = self.encoder(src_embedding)\n",
    "        decoded = self.decoder(tgt_embedding, encoded)\n",
    "\n",
    "        return self.output_layer(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd860be0-e82c-4d3c-a923-935176d601e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_losses = []\n",
    "def train_model(model, dataloader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {epoch + 1}:')\n",
    "        for input_seq, target_seq in pbar:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq, input_seq)\n",
    "            loss = criterion(output.view(-1, output.size(-1)), target_seq.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "        pbar.set_postfix(epoch_loss=epoch_loss)\n",
    "        epoch_losses.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884bab2-5698-4fc6-a86e-8bc7c4c0a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:: 100%|███████████████| 1563/1563 [05:23<00:00,  4.84it/s, loss=0.00912]\n",
      "Epoch 2:: 100%|███████████████| 1563/1563 [05:27<00:00,  4.77it/s, loss=0.00218]\n",
      "Epoch 3:: 100%|██████████████| 1563/1563 [05:30<00:00,  4.73it/s, loss=0.000789]\n",
      "Epoch 4::  49%|███████▍       | 772/1563 [02:49<03:07,  4.22it/s, loss=0.000472]"
     ]
    }
   ],
   "source": [
    "dataset = SQLDataset(generic_sqls, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss function\n",
    "VOCAB_SIZE = len(tokens_embedding)  # Example vocabulary size (depends on your tokenizer)\n",
    "model = TransformerAutoencoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_HEADS, NUM_LAYERS, MAX_SEQ_LEN, DROPOUT)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens in the loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, dataloader, criterion, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2869af4-b0fe-4129-a3cf-8debc52d20b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
